{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\cesar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\cesar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\cesar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\cesar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\cesar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\cesar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\users\\cesar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\cesar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\cesar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\cesar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\cesar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\cesar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import pandas as pd\n",
    "import pandas\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, LabelBinarizer\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formata os arquivos em dicionário na forma:\n",
    "# {'a': [0...[0...[0...30]...100]...10]}\n",
    "# 30 dados (gx, gy, gz, ax, ay, az para 5 dedos)\n",
    "# com 100 amostras para cada sinal\n",
    "# e realizado 10 vezes\n",
    "\n",
    "directory = 'Dados_alfabeto'\n",
    "letras = os.listdir(directory) \n",
    "num_letras = len(letras)\n",
    "\n",
    "dataset = pd.DataFrame()\n",
    "\n",
    "files = {}\n",
    "for letra in letras:\n",
    "    files[letra] = os.listdir(directory+'/'+letra)\n",
    "\n",
    "stats = []\n",
    "all_dataframe = {}\n",
    "    \n",
    "# formated_dict = {}\n",
    "for letra, value_list in files.items():\n",
    "#     formated_dict[letra] = []\n",
    "    for f in value_list:\n",
    "        with open(directory+\"/\"+letra+\"/\"+f) as file:\n",
    "            lines = [line.strip() for line in file]\n",
    "            lines = ast.literal_eval(lines[0])\n",
    "            test = pd.DataFrame(lines)\n",
    "            d = {'média':test.mean(), 'desvio_padrao': test.std(), 'max': test.max(), 'min': test.min()}\n",
    "\n",
    "            stats.append(pd.DataFrame(d))\n",
    "            test['label'] = letra\n",
    "            dataset = dataset.append(test, ignore_index = True)\n",
    "            \n",
    "            if letra not in all_dataframe:\n",
    "                all_dataframe[letra] = []\n",
    "            all_dataframe[letra].append(test)\n",
    "#             formated_dict[letra].append(lines)\n",
    "\n",
    "if not os.path.exists('./estatisticas'):\n",
    "    os.mkdir('./estatisticas')\n",
    "\n",
    "estatisticas = {}\n",
    "# letras_list = ['a','b','c','d','e','f','g','i','l','m','n','o','p','q','r','s','t','u','v','w']\n",
    "i = 0;\n",
    "for letr in letras:\n",
    "    estatisticas[letr] = stats[i:i+10]\n",
    "    i+=10\n",
    "    \n",
    "for key,list_values in estatisticas.items():\n",
    "    for i,v in enumerate(list_values):\n",
    "        v.to_excel('./estatisticas/' + key+'_'+str(i)+'.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:,0:30] # [all rows, col from index 2 to the last one excluding 'Unnamed: 32']\n",
    "y = dataset.iloc[:,30] # [all rows, col one only which contains the classes of cancer]\n",
    "\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "X = X/32768\n",
    "# X = StandardScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Tranform training labels to one-hot encoding\n",
    "y_train = np_utils.to_categorical(y_train, num_letras)\n",
    "\n",
    "# Tranform test labels to one-hot encoding\n",
    "y_test = np_utils.to_categorical(y_test, num_letras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_dic_conv_hist():\n",
    "    nome_dados = ['ax0', 'ay0', 'az0', 'gx0', 'gy0', 'gz0',\n",
    "                 'ax1', 'ay1', 'az1', 'gx1', 'gy1', 'gz1',\n",
    "                 'ax2', 'ay2', 'az2', 'gx2', 'gy2', 'gz2',\n",
    "                 'ax3', 'ay3', 'az3', 'gx3', 'gy3', 'gz3',\n",
    "                 'ax4', 'ay4', 'az4', 'gx4', 'gy4', 'gz4']\n",
    "\n",
    "    dic_conv_hist = {}\n",
    "    for i,nom in enumerate(nome_dados):\n",
    "        dic_conv_hist[i] = nom\n",
    "    return dic_conv_hist\n",
    "        \n",
    "\n",
    "def save_histogramas(all_dataframe, letras):\n",
    "    if not os.path.exists('./histogramas'):\n",
    "        os.mkdir('./histogramas')\n",
    "\n",
    "    for letra in letras:\n",
    "        if not os.path.exists('./histogramas/'+letra):\n",
    "            os.mkdir('./histogramas/'+letra)\n",
    "        for ex in range(10):\n",
    "            if not os.path.exists('./histogramas/'+letra+\"/\"+str(ex)):\n",
    "                os.mkdir('./histogramas/'+letra+\"/\"+str(ex))\n",
    "        \n",
    "    dic_conv_hist = gerar_dic_conv_hist()\n",
    "\n",
    "    for letra in letras:\n",
    "        for ex in range(10):\n",
    "            direc = './histogramas/'+letra+\"/\"+ str(ex) + \"/\"\n",
    "            for dado in range(30):\n",
    "                figure = all_dataframe[letra][ex].iloc[:,dado].hist(bins=100).get_figure()\n",
    "                plt.tight_layout()\n",
    "                figure.savefig(direc + letra + \"_\" + dic_conv_hist[dado] + \".png\")\n",
    "                plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_histogramas(all_dataframe, letras)\n",
    "# get_estatisticas_por_letra(estatisticas, letras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estatisticas_por_letra(estatisticas, letras):\n",
    "\n",
    "    est_letra = {}\n",
    "    for key,list_values in estatisticas.items():\n",
    "        est_letra[key] = pd.DataFrame()\n",
    "        for i,v in enumerate(list_values):\n",
    "            est_letra[key] = est_letra[key].append(pd.DataFrame(v))\n",
    "        est_letra[key] = est_letra[key].reset_index()\n",
    "\n",
    "    est_tmp = {}\n",
    "    dic_conv_hist = gerar_dic_conv_hist()\n",
    "\n",
    "    for letra in letras:\n",
    "        est_tmp[letra] = []\n",
    "        for i in range(30):\n",
    "            tmp_pd = est_letra[letra][i::30]\n",
    "            dic_tmp = {'desvio_padrao': tmp_pd['desvio_padrao'].std(ddof=0), \n",
    "                       'max': tmp_pd['max'].max(), \n",
    "                       'min': tmp_pd['min'].min(),\n",
    "                       'label': dic_conv_hist[i]\n",
    "                      }\n",
    "            est_tmp[letra].append(pd.DataFrame(dic_tmp, index=[i]))\n",
    "    \n",
    "    if not os.path.exists('./estatisticas_gerais'):\n",
    "        os.mkdir('./estatisticas_gerais')\n",
    "    \n",
    "    estatisticas_geral = {}\n",
    "    for letra in letras:\n",
    "        estatisticas_geral[letra] = pd.concat(est_tmp[letra])\n",
    "        estatisticas_geral[letra].to_excel('./estatisticas_gerais/'+letra+'.xlsx')\n",
    "        \n",
    "    return estatisticas_geral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "WARNING:tensorflow:From /home/cesar/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/cesar/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 15920 samples, validate on 3980 samples\n",
      "Epoch 1/100\n",
      "15920/15920 [==============================] - 3s 199us/step - loss: 2.6879 - acc: 0.3551 - val_loss: 2.2615 - val_acc: 0.5987\n",
      "Epoch 2/100\n",
      "15920/15920 [==============================] - 1s 84us/step - loss: 1.7440 - acc: 0.7180 - val_loss: 1.2918 - val_acc: 0.8704\n",
      "Epoch 3/100\n",
      "15920/15920 [==============================] - 1s 85us/step - loss: 0.9878 - acc: 0.8955 - val_loss: 0.7490 - val_acc: 0.9068\n",
      "Epoch 4/100\n",
      "15920/15920 [==============================] - 1s 85us/step - loss: 0.5956 - acc: 0.9295 - val_loss: 0.4666 - val_acc: 0.9731\n",
      "Epoch 5/100\n",
      "15920/15920 [==============================] - 1s 84us/step - loss: 0.3804 - acc: 0.9562 - val_loss: 0.3034 - val_acc: 0.9558\n",
      "Epoch 6/100\n",
      "15920/15920 [==============================] - 1s 93us/step - loss: 0.2581 - acc: 0.9671 - val_loss: 0.2131 - val_acc: 0.9774\n",
      "Epoch 7/100\n",
      "15920/15920 [==============================] - 1s 88us/step - loss: 0.1893 - acc: 0.9731 - val_loss: 0.1592 - val_acc: 0.9789\n",
      "Epoch 8/100\n",
      "15920/15920 [==============================] - 2s 98us/step - loss: 0.1471 - acc: 0.9782 - val_loss: 0.1273 - val_acc: 0.9721\n",
      "Epoch 9/100\n",
      "15920/15920 [==============================] - 2s 95us/step - loss: 0.1194 - acc: 0.9812 - val_loss: 0.1024 - val_acc: 0.9857\n",
      "Epoch 10/100\n",
      "15920/15920 [==============================] - 2s 99us/step - loss: 0.0990 - acc: 0.9851 - val_loss: 0.0851 - val_acc: 0.9877\n",
      "Epoch 11/100\n",
      "15920/15920 [==============================] - 2s 100us/step - loss: 0.0831 - acc: 0.9864 - val_loss: 0.0723 - val_acc: 0.9874\n",
      "Epoch 12/100\n",
      "15920/15920 [==============================] - 1s 89us/step - loss: 0.0704 - acc: 0.9886 - val_loss: 0.0590 - val_acc: 0.9905\n",
      "Epoch 13/100\n",
      "15920/15920 [==============================] - 1s 84us/step - loss: 0.0603 - acc: 0.9898 - val_loss: 0.0500 - val_acc: 0.9917\n",
      "Epoch 14/100\n",
      "15920/15920 [==============================] - 1s 90us/step - loss: 0.0514 - acc: 0.9918 - val_loss: 0.0428 - val_acc: 0.9917\n",
      "Epoch 15/100\n",
      "15920/15920 [==============================] - 2s 104us/step - loss: 0.0440 - acc: 0.9935 - val_loss: 0.0356 - val_acc: 0.9940\n",
      "Epoch 16/100\n",
      "15920/15920 [==============================] - 1s 92us/step - loss: 0.0378 - acc: 0.9955 - val_loss: 0.0302 - val_acc: 0.9957\n",
      "Epoch 17/100\n",
      "15920/15920 [==============================] - 1s 92us/step - loss: 0.0330 - acc: 0.9965 - val_loss: 0.0261 - val_acc: 0.9945\n",
      "Epoch 18/100\n",
      "15920/15920 [==============================] - 1s 87us/step - loss: 0.0289 - acc: 0.9966 - val_loss: 0.0214 - val_acc: 0.9972\n",
      "Epoch 19/100\n",
      "15920/15920 [==============================] - 1s 84us/step - loss: 0.0258 - acc: 0.9970 - val_loss: 0.0183 - val_acc: 0.9977\n",
      "Epoch 20/100\n",
      "15920/15920 [==============================] - 1s 86us/step - loss: 0.0226 - acc: 0.9976 - val_loss: 0.0160 - val_acc: 0.9982\n",
      "Epoch 21/100\n",
      "15920/15920 [==============================] - 1s 93us/step - loss: 0.0207 - acc: 0.9977 - val_loss: 0.0139 - val_acc: 0.9982\n",
      "Epoch 22/100\n",
      "15920/15920 [==============================] - 1s 84us/step - loss: 0.0188 - acc: 0.9977 - val_loss: 0.0121 - val_acc: 0.9987\n",
      "Epoch 23/100\n",
      "15920/15920 [==============================] - 1s 86us/step - loss: 0.0172 - acc: 0.9979 - val_loss: 0.0107 - val_acc: 0.9990\n",
      "Epoch 24/100\n",
      "15920/15920 [==============================] - 1s 87us/step - loss: 0.0160 - acc: 0.9981 - val_loss: 0.0100 - val_acc: 0.9987\n",
      "Epoch 25/100\n",
      "15920/15920 [==============================] - 1s 91us/step - loss: 0.0147 - acc: 0.9981 - val_loss: 0.0086 - val_acc: 0.9992\n",
      "Epoch 26/100\n",
      "15920/15920 [==============================] - 1s 83us/step - loss: 0.0141 - acc: 0.9982 - val_loss: 0.0078 - val_acc: 0.9990\n",
      "Epoch 27/100\n",
      "15920/15920 [==============================] - 1s 86us/step - loss: 0.0131 - acc: 0.9983 - val_loss: 0.0068 - val_acc: 0.9992\n",
      "Epoch 28/100\n",
      "15920/15920 [==============================] - 1s 86us/step - loss: 0.0124 - acc: 0.9984 - val_loss: 0.0064 - val_acc: 0.9992\n",
      "Epoch 29/100\n",
      "15920/15920 [==============================] - 1s 93us/step - loss: 0.0115 - acc: 0.9986 - val_loss: 0.0060 - val_acc: 0.9992\n",
      "Epoch 30/100\n",
      "15920/15920 [==============================] - 1s 85us/step - loss: 0.0110 - acc: 0.9987 - val_loss: 0.0054 - val_acc: 0.9992\n",
      "Epoch 31/100\n",
      "15920/15920 [==============================] - 1s 84us/step - loss: 0.0107 - acc: 0.9987 - val_loss: 0.0049 - val_acc: 0.9992\n",
      "Epoch 32/100\n",
      "15920/15920 [==============================] - 2s 96us/step - loss: 0.0103 - acc: 0.9987 - val_loss: 0.0047 - val_acc: 0.9992\n",
      "Epoch 33/100\n",
      "15920/15920 [==============================] - 2s 117us/step - loss: 0.0099 - acc: 0.9988 - val_loss: 0.0045 - val_acc: 0.9992\n",
      "Epoch 34/100\n",
      "15920/15920 [==============================] - 2s 97us/step - loss: 0.0094 - acc: 0.9989 - val_loss: 0.0041 - val_acc: 0.9992\n",
      "Epoch 35/100\n",
      "15920/15920 [==============================] - 1s 94us/step - loss: 0.0092 - acc: 0.9989 - val_loss: 0.0036 - val_acc: 0.9992\n",
      "Epoch 36/100\n",
      "15920/15920 [==============================] - 2s 100us/step - loss: 0.0087 - acc: 0.9989 - val_loss: 0.0035 - val_acc: 0.9992\n",
      "Epoch 37/100\n",
      "15920/15920 [==============================] - 2s 111us/step - loss: 0.0089 - acc: 0.9990 - val_loss: 0.0033 - val_acc: 0.9992\n",
      "Epoch 38/100\n",
      "15920/15920 [==============================] - 1s 91us/step - loss: 0.0085 - acc: 0.9991 - val_loss: 0.0031 - val_acc: 0.9992\n",
      "Epoch 39/100\n",
      "15920/15920 [==============================] - 2s 96us/step - loss: 0.0083 - acc: 0.9991 - val_loss: 0.0028 - val_acc: 0.9992\n",
      "Epoch 40/100\n",
      "15920/15920 [==============================] - 2s 99us/step - loss: 0.0082 - acc: 0.9991 - val_loss: 0.0026 - val_acc: 0.9992\n",
      "Epoch 41/100\n",
      "15920/15920 [==============================] - 2s 101us/step - loss: 0.0079 - acc: 0.9991 - val_loss: 0.0025 - val_acc: 0.9992\n",
      "Epoch 42/100\n",
      "15920/15920 [==============================] - 2s 95us/step - loss: 0.0078 - acc: 0.9992 - val_loss: 0.0025 - val_acc: 0.9992\n",
      "Epoch 43/100\n",
      "15920/15920 [==============================] - 2s 98us/step - loss: 0.0077 - acc: 0.9992 - val_loss: 0.0023 - val_acc: 0.9992\n",
      "Epoch 44/100\n",
      "15920/15920 [==============================] - 2s 96us/step - loss: 0.0076 - acc: 0.9991 - val_loss: 0.0023 - val_acc: 0.9992\n",
      "Epoch 45/100\n",
      "15920/15920 [==============================] - 2s 97us/step - loss: 0.0077 - acc: 0.9992 - val_loss: 0.0021 - val_acc: 0.9992\n",
      "Epoch 46/100\n",
      "15920/15920 [==============================] - 2s 96us/step - loss: 0.0075 - acc: 0.9992 - val_loss: 0.0019 - val_acc: 0.9992\n",
      "Epoch 47/100\n",
      "15920/15920 [==============================] - 2s 97us/step - loss: 0.0070 - acc: 0.9992 - val_loss: 0.0019 - val_acc: 0.9995\n",
      "Epoch 48/100\n",
      "15920/15920 [==============================] - 2s 96us/step - loss: 0.0074 - acc: 0.9991 - val_loss: 0.0019 - val_acc: 0.9995\n",
      "Epoch 49/100\n",
      "15920/15920 [==============================] - 2s 118us/step - loss: 0.0070 - acc: 0.9991 - val_loss: 0.0017 - val_acc: 0.9995\n",
      "Epoch 50/100\n",
      "15920/15920 [==============================] - 2s 121us/step - loss: 0.0067 - acc: 0.9991 - val_loss: 0.0016 - val_acc: 0.9995\n",
      "Epoch 51/100\n",
      "15920/15920 [==============================] - 2s 120us/step - loss: 0.0068 - acc: 0.9991 - val_loss: 0.0015 - val_acc: 0.9995\n",
      "Epoch 52/100\n",
      "15920/15920 [==============================] - 2s 119us/step - loss: 0.0065 - acc: 0.9992 - val_loss: 0.0014 - val_acc: 0.9995\n",
      "Epoch 53/100\n",
      "15920/15920 [==============================] - 2s 124us/step - loss: 0.0063 - acc: 0.9991 - val_loss: 0.0015 - val_acc: 0.9995\n",
      "Epoch 54/100\n",
      "15920/15920 [==============================] - 3s 183us/step - loss: 0.0061 - acc: 0.9991 - val_loss: 0.0015 - val_acc: 0.9995\n",
      "Epoch 55/100\n",
      "15920/15920 [==============================] - 2s 153us/step - loss: 0.0062 - acc: 0.9992 - val_loss: 0.0015 - val_acc: 0.9995\n",
      "Epoch 56/100\n",
      "15920/15920 [==============================] - 2s 136us/step - loss: 0.0063 - acc: 0.9992 - val_loss: 0.0015 - val_acc: 0.9995\n",
      "Epoch 57/100\n",
      "15920/15920 [==============================] - 2s 121us/step - loss: 0.0058 - acc: 0.9992 - val_loss: 0.0015 - val_acc: 0.9995\n",
      "Epoch 58/100\n",
      "15920/15920 [==============================] - 2s 111us/step - loss: 0.0060 - acc: 0.9992 - val_loss: 0.0014 - val_acc: 0.9995\n",
      "Epoch 59/100\n",
      "15920/15920 [==============================] - 2s 138us/step - loss: 0.0057 - acc: 0.9992 - val_loss: 0.0014 - val_acc: 0.9995\n",
      "Epoch 60/100\n",
      "15920/15920 [==============================] - 3s 159us/step - loss: 0.0058 - acc: 0.9992 - val_loss: 0.0013 - val_acc: 0.9995\n",
      "Epoch 61/100\n",
      "15920/15920 [==============================] - 3s 165us/step - loss: 0.0058 - acc: 0.9992 - val_loss: 0.0013 - val_acc: 0.9995\n",
      "Epoch 62/100\n",
      "15920/15920 [==============================] - 2s 138us/step - loss: 0.0057 - acc: 0.9992 - val_loss: 0.0013 - val_acc: 0.9995\n",
      "Epoch 63/100\n",
      "15920/15920 [==============================] - 2s 148us/step - loss: 0.0057 - acc: 0.9992 - val_loss: 0.0014 - val_acc: 0.9995\n",
      "Epoch 64/100\n",
      "15920/15920 [==============================] - 3s 158us/step - loss: 0.0055 - acc: 0.9992 - val_loss: 0.0014 - val_acc: 0.9995\n",
      "Epoch 65/100\n",
      "15920/15920 [==============================] - 3s 160us/step - loss: 0.0055 - acc: 0.9992 - val_loss: 0.0014 - val_acc: 0.9995\n",
      "Epoch 66/100\n",
      "15920/15920 [==============================] - 2s 153us/step - loss: 0.0053 - acc: 0.9992 - val_loss: 0.0014 - val_acc: 0.9995\n",
      "Epoch 67/100\n",
      "15920/15920 [==============================] - 3s 167us/step - loss: 0.0054 - acc: 0.9992 - val_loss: 0.0014 - val_acc: 0.9995\n",
      "Epoch 68/100\n",
      "15920/15920 [==============================] - 3s 158us/step - loss: 0.0051 - acc: 0.9993 - val_loss: 0.0013 - val_acc: 0.9995\n",
      "Epoch 69/100\n",
      "15920/15920 [==============================] - 2s 153us/step - loss: 0.0050 - acc: 0.9992 - val_loss: 0.0012 - val_acc: 0.9995\n",
      "Epoch 70/100\n",
      "15920/15920 [==============================] - 2s 114us/step - loss: 0.0050 - acc: 0.9992 - val_loss: 0.0014 - val_acc: 0.9995\n",
      "Epoch 71/100\n",
      "15920/15920 [==============================] - 2s 107us/step - loss: 0.0050 - acc: 0.9992 - val_loss: 0.0015 - val_acc: 0.9995\n",
      "Epoch 72/100\n",
      "15920/15920 [==============================] - 1s 83us/step - loss: 0.0052 - acc: 0.9992 - val_loss: 0.0013 - val_acc: 0.9995\n",
      "Epoch 73/100\n",
      "15920/15920 [==============================] - 2s 95us/step - loss: 0.0050 - acc: 0.9992 - val_loss: 0.0013 - val_acc: 0.9995\n",
      "Epoch 74/100\n",
      "15920/15920 [==============================] - 2s 117us/step - loss: 0.0049 - acc: 0.9992 - val_loss: 0.0014 - val_acc: 0.9995\n",
      "Epoch 75/100\n",
      "15920/15920 [==============================] - 2s 101us/step - loss: 0.0050 - acc: 0.9992 - val_loss: 0.0014 - val_acc: 0.9995\n",
      "Epoch 76/100\n",
      "15920/15920 [==============================] - 2s 104us/step - loss: 0.0049 - acc: 0.9993 - val_loss: 0.0015 - val_acc: 0.9995\n",
      "Epoch 77/100\n",
      "15920/15920 [==============================] - 2s 131us/step - loss: 0.0049 - acc: 0.9992 - val_loss: 0.0015 - val_acc: 0.9995\n",
      "Epoch 78/100\n",
      "15920/15920 [==============================] - 2s 134us/step - loss: 0.0048 - acc: 0.9994 - val_loss: 0.0015 - val_acc: 0.9995\n",
      "Epoch 79/100\n",
      "15920/15920 [==============================] - 2s 104us/step - loss: 0.0047 - acc: 0.9994 - val_loss: 0.0015 - val_acc: 0.9995\n",
      "Epoch 80/100\n",
      "15920/15920 [==============================] - 2s 153us/step - loss: 0.0045 - acc: 0.9993 - val_loss: 0.0016 - val_acc: 0.9992\n",
      "Epoch 81/100\n",
      "15920/15920 [==============================] - 2s 139us/step - loss: 0.0044 - acc: 0.9994 - val_loss: 0.0014 - val_acc: 0.9995\n",
      "Epoch 82/100\n",
      "15920/15920 [==============================] - 3s 164us/step - loss: 0.0042 - acc: 0.9993 - val_loss: 0.0016 - val_acc: 0.9995\n",
      "Epoch 83/100\n",
      "15920/15920 [==============================] - 2s 136us/step - loss: 0.0040 - acc: 0.9994 - val_loss: 0.0015 - val_acc: 0.9995\n",
      "Epoch 84/100\n",
      "15920/15920 [==============================] - 2s 144us/step - loss: 0.0039 - acc: 0.9994 - val_loss: 0.0015 - val_acc: 0.9995\n",
      "Epoch 85/100\n",
      "15920/15920 [==============================] - 3s 158us/step - loss: 0.0038 - acc: 0.9994 - val_loss: 0.0016 - val_acc: 0.9995\n",
      "Epoch 86/100\n",
      "15920/15920 [==============================] - 3s 158us/step - loss: 0.0040 - acc: 0.9993 - val_loss: 0.0016 - val_acc: 0.9995\n",
      "Epoch 87/100\n",
      "15920/15920 [==============================] - 3s 160us/step - loss: 0.0039 - acc: 0.9994 - val_loss: 0.0017 - val_acc: 0.9995\n",
      "Epoch 88/100\n",
      "15920/15920 [==============================] - 2s 148us/step - loss: 0.0038 - acc: 0.9994 - val_loss: 0.0017 - val_acc: 0.9995\n",
      "Epoch 89/100\n",
      "15920/15920 [==============================] - 2s 152us/step - loss: 0.0040 - acc: 0.9993 - val_loss: 0.0017 - val_acc: 0.9995\n",
      "Epoch 90/100\n",
      "15920/15920 [==============================] - 2s 128us/step - loss: 0.0041 - acc: 0.9993 - val_loss: 0.0017 - val_acc: 0.9995\n",
      "Epoch 91/100\n",
      "15920/15920 [==============================] - 2s 119us/step - loss: 0.0041 - acc: 0.9994 - val_loss: 0.0017 - val_acc: 0.9995\n",
      "Epoch 92/100\n",
      "15920/15920 [==============================] - 2s 149us/step - loss: 0.0037 - acc: 0.9994 - val_loss: 0.0018 - val_acc: 0.9995\n",
      "Epoch 93/100\n",
      "15920/15920 [==============================] - 2s 126us/step - loss: 0.0040 - acc: 0.9994 - val_loss: 0.0017 - val_acc: 0.9995\n",
      "Epoch 94/100\n",
      "15920/15920 [==============================] - 2s 143us/step - loss: 0.0038 - acc: 0.9994 - val_loss: 0.0018 - val_acc: 0.9995\n",
      "Epoch 95/100\n",
      "15920/15920 [==============================] - 2s 126us/step - loss: 0.0037 - acc: 0.9994 - val_loss: 0.0017 - val_acc: 0.9995\n",
      "Epoch 96/100\n",
      "15920/15920 [==============================] - 2s 124us/step - loss: 0.0037 - acc: 0.9993 - val_loss: 0.0019 - val_acc: 0.9995\n",
      "Epoch 97/100\n",
      "15920/15920 [==============================] - 2s 127us/step - loss: 0.0036 - acc: 0.9994 - val_loss: 0.0018 - val_acc: 0.9995\n",
      "Epoch 98/100\n",
      "15920/15920 [==============================] - 2s 123us/step - loss: 0.0038 - acc: 0.9994 - val_loss: 0.0018 - val_acc: 0.9995\n",
      "Epoch 99/100\n",
      "15920/15920 [==============================] - 2s 125us/step - loss: 0.0035 - acc: 0.9994 - val_loss: 0.0018 - val_acc: 0.9995\n",
      "Epoch 100/100\n",
      "15920/15920 [==============================] - 2s 134us/step - loss: 0.0034 - acc: 0.9995 - val_loss: 0.0017 - val_acc: 0.9995\n",
      "3980/3980 [==============================] - 0s 26us/step\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "classes = num_letras\n",
    "hidden_layers = 0\n",
    "neurons = [25] * hidden_layers\n",
    "print(neurons)\n",
    "epochs = 100\n",
    "\n",
    "# Camada de entrada\n",
    "model.add(Dense(units=25, activation='sigmoid', input_dim=30))\n",
    "\n",
    "# model.add(Dropout(0.2))\n",
    "# Camadas escondidas\n",
    "for i in range(hidden_layers):\n",
    "    model.add(Dense(units=neurons[i], activation='sigmoid'))\n",
    "\n",
    "# Camada de saída\\\n",
    "model.add(Dense(units=classes, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size = 20, validation_data=(X_test, y_test), verbose=1)\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# model.save('modelo_alfabeto_treinado.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('modelo_alfabeto_treinado.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_acc_results(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_loss_results(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('modelo_alfabeto_treinado.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "errors = test_file(model, 'w_2019-09-18 17_42_23.384929.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gera um dicionário de mapeamento de letra em valor inteiro (a->0, b->1, ..., w->20)\n",
    "dic_conv = {}\n",
    "for i,letra in enumerate(letras):\n",
    "    dic_conv[letra] = i\n",
    "    \n",
    "# dic_conv = {'a':0, 'b':1, 'c':2, 'd':3, 'e':4, 'f':5, 'g':6, 'i':7,\n",
    "#                'l':8, 'm':9, 'n':10, 'o':11, 'p':12, 'q':13, 'r':14, 's':15,\n",
    "#                't':16, 'u':17, 'v':18, 'w':19}\n",
    "\n",
    "\n",
    "def indices_to_one_hot(letra, nb_classes):\n",
    "    num = dic_conv[letra]\n",
    "    return [[0 if i!=num else 1 for i in range(nb_classes)]]\n",
    "\n",
    "\n",
    "def get_incorrects(model, data, labels):\n",
    "    p = model.predict(data)\n",
    "    \n",
    "    errors = []\n",
    "    for i in range(len(data)):\n",
    "        equal = (np.where(labels[i] == np.amax(labels[i]))[0][0]) == (np.where(p[i] == np.amax(p[i])))[0][0]\n",
    "        if not equal:\n",
    "            print((np.where(labels[i] == np.amax(labels[i]))[0][0]), (np.where(p[i] == np.amax(p[i])))[0][0])\n",
    "            print()\n",
    "            errors.append(i)\n",
    "        \n",
    "    print(len(errors))\n",
    "    return errors\n",
    "\n",
    "\n",
    "def test_file(model, file_name):\n",
    "    letra = file_name[0].lower()\n",
    "    dataset_test = pd.DataFrame()\n",
    "\n",
    "    with open(file_name) as file:\n",
    "        lines_test = [l.strip() for l in file]\n",
    "        lines_test = ast.literal_eval(lines_test[0])\n",
    "        temp = pd.DataFrame(lines_test)\n",
    "        temp['label'] = 'c'\n",
    "        dataset_test = dataset_test.append(temp, ignore_index = True)\n",
    "\n",
    "    X_DT = dataset_test.iloc[:,0:30] # [all rows, col from index 2 to the last one excludind 'label']\n",
    "    X_DT = X_DT/32768\n",
    "\n",
    "    # Tranform training labels to one-hot encoding\n",
    "    y_DT = indices_to_one_hot(letra,20)\n",
    "\n",
    "    y_DT = y_DT*100\n",
    "    \n",
    "    errors = get_incorrects(model, X_DT, y_DT)\n",
    "    return errors\n",
    "\n",
    "\n",
    "def print_acc_results(history):\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "#     plt.title('Resultados no treinamento')\n",
    "    plt.ylabel('Acurácia')\n",
    "    plt.xlabel('Época')\n",
    "    plt.legend(['Treinamento', 'Teste'], loc='best')\n",
    "    plt.grid()\n",
    "#     plt.ylim((0, 1)) \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_loss_results(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "#     plt.title('Resultados no teste')\n",
    "    plt.ylabel('Entropia Cruzada')\n",
    "    plt.xlabel('Época')\n",
    "    plt.legend(['Treinamento', 'Teste'], loc='best')\n",
    "    plt.grid()\n",
    "#     plt.ylim((0, 10)) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = 'a_2019-09-14 18_12_10.193597.txt'\n",
    "# model = load_model('modelo_alfabeto_treinado.h5')\n",
    "# letra = file_name[0].lower()\n",
    "# dataset_test = pd.DataFrame()\n",
    "\n",
    "# with open(file_name) as file:\n",
    "#     lines_test = [l.strip() for l in file]\n",
    "#     lines_test = ast.literal_eval(lines_test[0])\n",
    "#     temp = pd.DataFrame(lines_test)\n",
    "#     temp['label'] = 'c'\n",
    "#     dataset_test = dataset_test.append(temp, ignore_index = True)\n",
    "\n",
    "# X_DT = dataset_test.iloc[:,0:30] # [all rows, col from index 2 to the last one excludind 'label']\n",
    "# X_DT = X_DT/32768\n",
    "\n",
    "# # Tranform training labels to one-hot encoding\n",
    "# y_DT = indices_to_one_hot(letra,20)\n",
    "\n",
    "# y_DT = y_DT*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cesar/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/lite.py:591: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.convert_variables_to_constants\n",
      "WARNING:tensorflow:From /home/cesar/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.extract_sub_graph\n",
      "INFO:tensorflow:Froze 4 variables.\n",
      "INFO:tensorflow:Converted 4 variables to const ops.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6396"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.contrib import lite\n",
    "converter = lite.TFLiteConverter.from_keras_model_file('modelo_alfabeto_treinado.h5') # Your model's name\n",
    "model = converter.convert()\n",
    "file = open( 'model.tflite' , 'wb' ) \n",
    "file.write( model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
    "    \"\"\"\n",
    "    Freezes the state of a session into a pruned computation graph.\n",
    "\n",
    "    Creates a new computation graph where variable nodes are replaced by\n",
    "    constants taking their current value in the session. The new graph will be\n",
    "    pruned so subgraphs that are not necessary to compute the requested\n",
    "    outputs are removed.\n",
    "    @param session The TensorFlow session to be frozen.\n",
    "    @param keep_var_names A list of variable names that should not be frozen,\n",
    "                          or None to freeze all the variables in the graph.\n",
    "    @param output_names Names of the relevant graph outputs.\n",
    "    @param clear_devices Remove the device directives from the graph for better portability.\n",
    "    @return The frozen graph definition.\n",
    "    \"\"\"\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.global_variables()]\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "        frozen_graph = tf.graph_util.convert_variables_to_constants(\n",
    "            session, input_graph_def, output_names, freeze_var_names)\n",
    "        return frozen_graph\n",
    "    \n",
    "\n",
    "model =  load_model('modelo_alfabeto_treinado.h5')\n",
    "    \n",
    "# Create, compile and train model...\n",
    "\n",
    "frozen_graph = freeze_session(K.get_session(),\n",
    "                              output_names=[out.op.name for out in model.outputs])\n",
    "\n",
    "tf.train.write_graph(frozen_graph, \"./\", \"my_model.pb\", as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tag_constants' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-54a14ff85f3a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mexport_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m   \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtag_constants\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAINING\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexport_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tag_constants' is not defined"
     ]
    }
   ],
   "source": [
    "export_dir = './'\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "  tf.saved_model.loader.load(sess, [tag_constants.TRAINING], export_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Train loss: 2.16 Train acc: 0.914, Test acc:0.909\n",
      "Epoch:1, Train loss: 2.12 Train acc: 0.895, Test acc:0.892\n",
      "Epoch:2, Train loss: 2.11 Train acc: 0.888, Test acc:0.886\n",
      "Epoch:3, Train loss: 2.11 Train acc: 0.934, Test acc:0.939\n",
      "Epoch:4, Train loss: 2.11 Train acc: 0.889, Test acc:0.897\n",
      "Epoch:5, Train loss: 2.11 Train acc: 0.918, Test acc:0.922\n",
      "Epoch:6, Train loss: 2.10 Train acc: 0.926, Test acc:0.928\n",
      "Epoch:7, Train loss: 2.10 Train acc: 0.938, Test acc:0.942\n",
      "Epoch:8, Train loss: 2.10 Train acc: 0.936, Test acc:0.939\n",
      "Epoch:9, Train loss: 2.10 Train acc: 0.938, Test acc:0.941\n",
      "Epoch:10, Train loss: 2.10 Train acc: 0.987, Test acc:0.987\n",
      "Epoch:11, Train loss: 2.09 Train acc: 0.989, Test acc:0.989\n",
      "Epoch:12, Train loss: 2.09 Train acc: 0.989, Test acc:0.989\n",
      "Epoch:13, Train loss: 2.09 Train acc: 0.989, Test acc:0.989\n",
      "Epoch:14, Train loss: 2.09 Train acc: 0.990, Test acc:0.989\n",
      "Epoch:15, Train loss: 2.09 Train acc: 0.989, Test acc:0.989\n",
      "Epoch:16, Train loss: 2.09 Train acc: 0.990, Test acc:0.989\n",
      "Epoch:17, Train loss: 2.09 Train acc: 0.991, Test acc:0.991\n",
      "Epoch:18, Train loss: 2.09 Train acc: 0.991, Test acc:0.991\n",
      "Epoch:19, Train loss: 2.09 Train acc: 0.991, Test acc:0.991\n",
      "Epoch:20, Train loss: 2.09 Train acc: 0.991, Test acc:0.991\n",
      "Epoch:21, Train loss: 2.09 Train acc: 0.991, Test acc:0.991\n",
      "Epoch:22, Train loss: 2.09 Train acc: 0.991, Test acc:0.991\n",
      "Epoch:23, Train loss: 2.09 Train acc: 0.991, Test acc:0.991\n",
      "Epoch:24, Train loss: 2.09 Train acc: 0.991, Test acc:0.992\n",
      "Epoch:25, Train loss: 2.09 Train acc: 0.991, Test acc:0.991\n",
      "Epoch:26, Train loss: 2.09 Train acc: 0.991, Test acc:0.992\n",
      "Epoch:27, Train loss: 2.09 Train acc: 0.991, Test acc:0.992\n",
      "Epoch:28, Train loss: 2.09 Train acc: 0.992, Test acc:0.992\n",
      "Epoch:29, Train loss: 2.09 Train acc: 0.991, Test acc:0.992\n",
      "Epoch:30, Train loss: 2.09 Train acc: 0.992, Test acc:0.992\n",
      "Epoch:31, Train loss: 2.09 Train acc: 0.992, Test acc:0.992\n",
      "Epoch:32, Train loss: 2.09 Train acc: 0.992, Test acc:0.992\n",
      "Epoch:33, Train loss: 2.09 Train acc: 0.992, Test acc:0.992\n",
      "Epoch:34, Train loss: 2.09 Train acc: 0.992, Test acc:0.992\n",
      "Epoch:35, Train loss: 2.09 Train acc: 0.992, Test acc:0.992\n",
      "Epoch:36, Train loss: 2.09 Train acc: 0.992, Test acc:0.992\n",
      "Epoch:37, Train loss: 2.09 Train acc: 0.993, Test acc:0.993\n",
      "Epoch:38, Train loss: 2.09 Train acc: 0.993, Test acc:0.993\n",
      "Epoch:39, Train loss: 2.09 Train acc: 0.993, Test acc:0.994\n",
      "Epoch:40, Train loss: 2.09 Train acc: 0.993, Test acc:0.994\n",
      "Epoch:41, Train loss: 2.09 Train acc: 0.993, Test acc:0.995\n",
      "Epoch:42, Train loss: 2.09 Train acc: 0.994, Test acc:0.995\n",
      "Epoch:43, Train loss: 2.09 Train acc: 0.994, Test acc:0.995\n",
      "Epoch:44, Train loss: 2.09 Train acc: 0.994, Test acc:0.995\n",
      "Epoch:45, Train loss: 2.09 Train acc: 0.994, Test acc:0.995\n",
      "Epoch:46, Train loss: 2.09 Train acc: 0.994, Test acc:0.995\n",
      "Epoch:47, Train loss: 2.09 Train acc: 0.994, Test acc:0.996\n",
      "Epoch:48, Train loss: 2.09 Train acc: 0.994, Test acc:0.995\n",
      "Epoch:49, Train loss: 2.09 Train acc: 0.994, Test acc:0.996\n"
     ]
    }
   ],
   "source": [
    "s = tf.InteractiveSession()\n",
    "\n",
    "num_classes = y_train.shape[1]\n",
    "num_features = X_train.shape[1]\n",
    "num_output = y_train.shape[1]\n",
    "num_layers_0 = 200\n",
    "starter_learning_rate = 0.01\n",
    "regularizer_rate = 0.5\n",
    "\n",
    "# Placeholders for the input data\n",
    "input_X = tf.placeholder('float32',shape =(None,num_features),name=\"input_X\")\n",
    "input_y = tf.placeholder('float32',shape = (None,num_classes),name='input_Y')\n",
    "## for dropout layer\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "## Weights initialized by random normal function with std_dev = 1/sqrt(number of input features)\n",
    "weights_0 = tf.Variable(tf.random_normal([num_features,num_layers_0], stddev=(1/tf.sqrt(float(num_features)))))\n",
    "bias_0 = tf.Variable(tf.random_normal([num_layers_0]))\n",
    "weights_2 = tf.Variable(tf.random_normal([num_layers_0,num_output], stddev=(1/tf.sqrt(float(num_layers_0)))))\n",
    "bias_2 = tf.Variable(tf.random_normal([num_output]))\n",
    "\n",
    "## Initializing weigths and biases\n",
    "hidden_output_0 = tf.nn.sigmoid(tf.matmul(input_X,weights_0)+bias_0)\n",
    "hidden_output_0_0 = tf.nn.dropout(hidden_output_0, keep_prob)\n",
    "predicted_y = tf.sigmoid(tf.matmul(hidden_output_0_0,weights_2) + bias_2)\n",
    "\n",
    "\n",
    "## Defining the loss function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=predicted_y,labels=input_y)) \\\n",
    "        + regularizer_rate*(tf.reduce_sum(tf.square(bias_0)))\n",
    "\n",
    "## Variable learning rate\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, 0, 5, 0.85, staircase=True)\n",
    "## Adam optimzer for finding the right weight\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss,var_list=[weights_0,weights_2,\n",
    "                                                                         bias_0,bias_2])\n",
    "# tf.train.RMSPropOptimizer\n",
    "## Metrics definition\n",
    "correct_prediction = tf.equal(tf.argmax(y_train,1), tf.argmax(predicted_y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "## Training parameters\n",
    "batch_size = 20\n",
    "epochs=50\n",
    "dropout_prob = 0.3\n",
    "training_accuracy = []\n",
    "training_loss = []\n",
    "testing_accuracy = []\n",
    "s.run(tf.global_variables_initializer())\n",
    "for epoch in range(epochs):    \n",
    "    arr = np.arange(X_train.shape[0])\n",
    "    np.random.shuffle(arr)\n",
    "    for index in range(0,X_train.shape[0],batch_size):\n",
    "        s.run(optimizer, {input_X: X_train[arr[index:index+batch_size]],\n",
    "                          input_y: y_train[arr[index:index+batch_size]],\n",
    "                        keep_prob:dropout_prob})\n",
    "    training_accuracy.append(s.run(accuracy, feed_dict= {input_X:X_train, \n",
    "                                                         input_y: y_train,keep_prob:1}))\n",
    "    training_loss.append(s.run(loss, {input_X: X_train, \n",
    "                                      input_y: y_train,keep_prob:1}))\n",
    "    \n",
    "    ## Evaluation of model\n",
    "    testing_accuracy.append(accuracy_score(y_test.argmax(1), \n",
    "                            s.run(predicted_y, {input_X: X_test,keep_prob:1}).argmax(1)))\n",
    "    print(\"Epoch:{0}, Train loss: {1:.2f} Train acc: {2:.3f}, Test acc:{3:.3f}\".format(epoch,\n",
    "                                                                    training_loss[epoch],\n",
    "                                                                    training_accuracy[epoch],\n",
    "                                                                   testing_accuracy[epoch]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.run(predicted_y, {input_X: [X_train[15]],keep_prob:1}).argmax(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        5.9604645e-08, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 9.9999923e-01,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_y.eval(feed_dict = {input_X:[X_train[19]], keep_prob:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'a_2019-09-14 18_12_10.193597.txt'\n",
    "letra = file_name[0].lower()\n",
    "dataset_test = pd.DataFrame()\n",
    "\n",
    "with open(file_name) as file:\n",
    "    lines_test = [l.strip() for l in file]\n",
    "    lines_test = ast.literal_eval(lines_test[0])\n",
    "    temp = pd.DataFrame(lines_test)\n",
    "    temp['label'] = 'c'\n",
    "    dataset_test = dataset_test.append(temp, ignore_index = True)\n",
    "\n",
    "X_DT = dataset_test.iloc[:,0:30] # [all rows, col from index 2 to the last one excludind 'label']\n",
    "X_DT = X_DT/32768\n",
    "\n",
    "# Tranform training labels to one-hot encoding\n",
    "y_DT = indices_to_one_hot(letra,20)\n",
    "\n",
    "y_DT = y_DT*100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    -0.040283\n",
       "1    -0.297363\n",
       "2     0.388062\n",
       "3    -0.024200\n",
       "4     0.000702\n",
       "5     0.000549\n",
       "6     0.186401\n",
       "7     0.415771\n",
       "8     0.227661\n",
       "9    -0.006592\n",
       "10    0.003632\n",
       "11   -0.011932\n",
       "12    0.209839\n",
       "13    0.438843\n",
       "14    0.135132\n",
       "15   -0.014801\n",
       "16    0.000610\n",
       "17   -0.004242\n",
       "18    0.265991\n",
       "19    0.399780\n",
       "20    0.124146\n",
       "21   -0.008392\n",
       "22   -0.007233\n",
       "23   -0.008881\n",
       "24    0.184937\n",
       "25    0.451660\n",
       "26    0.010498\n",
       "27   -0.014160\n",
       "28   -0.001068\n",
       "29   -0.013245\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_DT.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "acertos = 0\n",
    "for i in range(100):\n",
    "    output = s.run(predicted_y, {input_X:[X_DT.iloc[i]], keep_prob:1})\n",
    "    output = np.argmax(output)\n",
    "    if output == 0:\n",
    "        acertos+=1\n",
    "print(acertos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cesar/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/cesar/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"modelo_alfabeto_treinado.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 30]\n",
      "<class 'numpy.float32'>\n",
      "[ 1 20]\n",
      "<class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Print input shape and type\n",
    "print(interpreter.get_input_details()[0]['shape'])  # Example: [1 224 224 3]\n",
    "print(interpreter.get_input_details()[0]['dtype'])  # Example: <class 'numpy.float32'>\n",
    "\n",
    "# Print output shape and type\n",
    "print(interpreter.get_output_details()[0]['shape'])  # Example: [1 1000]\n",
    "print(interpreter.get_output_details()[0]['dtype'])  # Example: <class 'numpy.float32'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
