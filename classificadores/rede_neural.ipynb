{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cesar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\cesar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\cesar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\cesar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\cesar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\cesar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\users\\cesar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\cesar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\cesar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\cesar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\cesar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\cesar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "import pickle\n",
    "import random\n",
    "# from tensorflow.contrib import lite\n",
    "# from keras.utils import  plot_model, np_utils, to_categorical,\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Dense\n",
    "from keras.models import load_model, Sequential\n",
    "from keras import backend as K\n",
    "from random import randrange\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix #, roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.preprocessing import LabelEncoder, StandardScaler, LabelBinarizer\n",
    "\n",
    "# from bar import RBFLayer, InitCentersKMeans\n",
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"dataset_train.csv\")\n",
    "X_val = pd.read_csv('dataset_validation.csv')\n",
    "X_test = pd.read_csv('dataset_test.csv')\n",
    "\n",
    "X_train = X_train.iloc[:,1:31]\n",
    "X_val = X_val.iloc[:,1:31]\n",
    "X_test = X_test.iloc[:,1:31]\n",
    "\n",
    "y_train = np.loadtxt(\"labels_train.csv\", delimiter=\",\")\n",
    "y_val = np.loadtxt(\"labels_validation.csv\", delimiter=\",\")\n",
    "y_test = np.loadtxt(\"labels_test.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historicos = {}\n",
    "for neurons in range(1,21):\n",
    "    print(\"Iniciando treino com \", neurons, \" neurônios.\")\n",
    "    model = tf.compat.v2.keras.Sequential()\n",
    "\n",
    "    classes = len(y_train[0])\n",
    "#     neurons = 20\n",
    "    epochs = 1000\n",
    "\n",
    "    # Camada de entrada\n",
    "    model.add(tf.compat.v2.keras.layers.Dense(units=neurons, activation='sigmoid', input_dim=30))\n",
    "\n",
    "    model.add(tf.compat.v2.keras.layers.Dense(units=classes, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size = 20, validation_data=(X_val, y_val), verbose=1)\n",
    "#     loss, accuracy = model.evaluate(X_val, y_val)\n",
    "    loss_test, accuracy_test = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    lst_hist = history.history\n",
    "    tmp = [(lst_hist['acc'][-1],lst_hist['val_acc'][-1],accuracy_test), \n",
    "           (lst_hist['loss'][-1],lst_hist['val_loss'][-1],loss_test)]\n",
    "    historicos[neurons] = tmp\n",
    "    \n",
    "    if not os.path.exists('variacao_neuronios_sigmoide'):\n",
    "            os.mkdir('variacao_neuronios_sigmoide')\n",
    "    \n",
    "    with open('variacao_neuronios_sigmoide/'+ str(neurons) +'_history.pickle', 'wb') as fp:\n",
    "        pickle.dump(history.history, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('variacao_neuronios_sigmoide/'+ str(neurons) +'_tuplas_finais.pickle', 'wb') as fp:\n",
    "        pickle.dump(tmp, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    model.save('variacao_neuronios_sigmoide/'+ str(neurons) +'_network.h5')\n",
    "\n",
    "#     with open('variacao_neuronios/' + str(neurons) + '_neurons.pickle', 'wb') as handle:\n",
    "#         pickle.dump(history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# model.save('modelo_alfabeto_completo.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historicos = {}\n",
    "for neurons in range(2,21):\n",
    "    print(\"Iniciando treino com \", neurons, \" neurônios.\")\n",
    "    model = tf.compat.v2.keras.Sequential()\n",
    "\n",
    "    classes = len(y_train[0])\n",
    "#     neurons = 20\n",
    "    epochs = 1000\n",
    "\n",
    "    # Camada de entrada\n",
    "    model.add(tf.compat.v2.keras.layers.Dense(units=neurons, activation='relu', input_dim=30))\n",
    "\n",
    "    model.add(tf.compat.v2.keras.layers.Dense(units=classes, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size = 20, validation_data=(X_val, y_val), verbose=1)\n",
    "#     loss, accuracy = model.evaluate(X_val, y_val)\n",
    "    loss_test, accuracy_test = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    lst_hist = history.history\n",
    "    tmp = [(lst_hist['acc'][-1],lst_hist['val_acc'][-1],accuracy_test), \n",
    "           (lst_hist['loss'][-1],lst_hist['val_loss'][-1],loss_test)]\n",
    "    historicos[neurons] = tmp\n",
    "    \n",
    "    if not os.path.exists('variacao_neuronios_relu'):\n",
    "            os.mkdir('variacao_neuronios_relu')\n",
    "    \n",
    "    with open('variacao_neuronios_relu/'+ str(neurons) +'_history.pickle', 'wb') as fp:\n",
    "        pickle.dump(history.history, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('variacao_neuronios_relu/'+ str(neurons) +'_tuplas_finais.pickle', 'wb') as fp:\n",
    "        pickle.dump(tmp, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    model.save('variacao_neuronios_relu/'+ str(neurons) +'_network.h5')\n",
    "\n",
    "#     with open('variacao_neuronios/' + str(neurons) + '_neurons.pickle', 'wb') as handle:\n",
    "#         pickle.dump(history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# model.save('modelo_alfabeto_completo.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dic_conv = {'a':0, 'b':1, 'c':2, 'd':3, 'e':4, 'f':5, 'g':6, 'i':7,\n",
    "#                'l':8, 'm':9, 'n':10, 'o':11, 'p':12, 'q':13, 'r':14, 's':15,\n",
    "#                't':16, 'u':17, 'v':18, 'w':19}\n",
    "\n",
    "\n",
    "def indices_to_one_hot(letra, nb_classes):\n",
    "    # Gera um dicionário de mapeamento de letra em valor inteiro (a->0, b->1, ..., w->20)\n",
    "    dic_conv = {}\n",
    "    for i,letra in enumerate(letras):\n",
    "        dic_conv[letra] = i\n",
    "    \n",
    "    num = dic_conv[letra]\n",
    "    return [[0 if i!=num else 1 for i in range(nb_classes)]]\n",
    "\n",
    "\n",
    "def get_incorrects(model, data, labels):\n",
    "    p = model.predict(data)\n",
    "    \n",
    "    errors = []\n",
    "    for i in range(len(data)):\n",
    "        equal = (np.where(labels[i] == np.amax(labels[i]))[0][0]) == (np.where(p[i] == np.amax(p[i])))[0][0]\n",
    "        if not equal:\n",
    "            print((np.where(labels[i] == np.amax(labels[i]))[0][0]), (np.where(p[i] == np.amax(p[i])))[0][0])\n",
    "            print()\n",
    "            errors.append(i)\n",
    "        \n",
    "    print(len(errors))\n",
    "    return errors\n",
    "\n",
    "\n",
    "def print_acc_results(history):\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "#     plt.title('Resultados no treinamento')\n",
    "    plt.ylabel('Acurácia')\n",
    "    plt.xlabel('Época')\n",
    "    plt.legend(['Treinamento', 'Teste'], loc='best')\n",
    "    plt.grid()\n",
    "#     plt.ylim((0, 1)) \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_loss_results(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "#     plt.title('Resultados no teste')\n",
    "    plt.ylabel('Entropia Cruzada')\n",
    "    plt.xlabel('Época')\n",
    "    plt.legend(['Treinamento', 'Teste'], loc='best')\n",
    "    plt.grid()\n",
    "#     plt.ylim((0, 10)) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model('modelo_alfabeto_treinado.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v2.keras import backend as K\n",
    "from tensorflow.compat.v2.keras.layers import Layer\n",
    "from tensorflow.compat.v2.keras.initializers import RandomUniform, Initializer, Constant\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "class InitCentersRandom(Initializer):\n",
    "    \"\"\" Initializer for initialization of centers of RBF network\n",
    "        as random samples from the given data set.\n",
    "    # Arguments\n",
    "        X: matrix, dataset to choose the centers from (random rows\n",
    "          are taken as centers)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "\n",
    "    def __call__(self, shape, dtype=None):\n",
    "        assert shape[1] == self.X.shape[1]\n",
    "        idx = np.random.randint(self.X.shape[0], size=shape[0])\n",
    "        return self.X[idx, :]\n",
    "\n",
    "\n",
    "class RBFLayer(Layer):\n",
    "    \"\"\" Layer of Gaussian RBF units.\n",
    "    # Example\n",
    "    ```python\n",
    "        model = Sequential()\n",
    "        model.add(RBFLayer(10,\n",
    "                           initializer=InitCentersRandom(X),\n",
    "                           betas=1.0,\n",
    "                           input_shape=(1,)))\n",
    "        model.add(Dense(1))\n",
    "    ```\n",
    "    # Arguments\n",
    "        output_dim: number of hidden units (i.e. number of outputs of the\n",
    "                    layer)\n",
    "        initializer: instance of initiliazer to initialize centers\n",
    "        betas: float, initial value for betas\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_dim, initializer=None, betas=1.0, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        self.init_betas = betas\n",
    "        self.initializer = initializer\n",
    "#         if not initializer:\n",
    "#             self.initializer = RandomUniform(0.0, 1.0)\n",
    "#         else:\n",
    "#             self.initializer = initializer\n",
    "        super(RBFLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        self.centers = self.add_weight(name='centers',\n",
    "                                       shape=(self.output_dim, input_shape[1]),\n",
    "                                       initializer=self.initializer,\n",
    "                                       trainable=True)\n",
    "        self.betas = self.add_weight(name='betas',\n",
    "                                     shape=(self.output_dim,),\n",
    "                                     initializer=Constant(\n",
    "                                         value=self.init_betas),\n",
    "                                     # initializer='ones',\n",
    "                                     trainable=True)\n",
    "\n",
    "        super(RBFLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "\n",
    "        C = K.expand_dims(self.centers)\n",
    "        H = K.transpose(C-K.transpose(x))\n",
    "        return K.exp(-self.betas * K.sum(H**2, axis=1))\n",
    "\n",
    "        # C = self.centers[np.newaxis, :, :]\n",
    "        # X = x[:, np.newaxis, :]\n",
    "\n",
    "        # diffnorm = K.sum((C-X)**2, axis=-1)\n",
    "        # ret = K.exp( - self.betas * diffnorm)\n",
    "        # return ret\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "\n",
    "    def get_config(self):\n",
    "        # have to define get_config to be able to use model_from_json\n",
    "        config = {\n",
    "            'output_dim': self.output_dim\n",
    "        }\n",
    "        base_config = super(RBFLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class InitCentersKMeans(Initializer):\n",
    "    \"\"\" Initializer for initialization of centers of RBF network\n",
    "        by clustering the given data set.\n",
    "    # Arguments\n",
    "        X: matrix, dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, max_iter=100):\n",
    "        self.X = X\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def __call__(self, shape, dtype=None):\n",
    "        assert shape[1] == self.X.shape[1]\n",
    "        n_centers = shape[0]\n",
    "        km = KMeans(n_clusters=n_centers, max_iter=self.max_iter, verbose=0)\n",
    "        km.fit(self.X)\n",
    "        return km.cluster_centers_\n",
    "    \n",
    "\n",
    "class GetCentersKMeans(Initializer):\n",
    "    \"\"\" Initializer for initialization of centers of RBF network\n",
    "        by clustering the given data set.\n",
    "    # Arguments\n",
    "        X: matrix, dataset\n",
    "    \"\"\"\n",
    "    \n",
    "#     def __init__(self, X):\n",
    "#         self.X = X\n",
    "\n",
    "    def __call__(self, shape, dtype=None):\n",
    "#         assert shape[1] == self.X.shape[1]\n",
    "        n_centers = shape[0]\n",
    "        centroides = np.loadtxt(\"k-means centroides/aleatorios/\"+str(n_centers)+\"_clusters.csv\", delimiter=\",\")\n",
    "        if not len(centroides.shape)==2:\n",
    "            centroides = np.expand_dims(centroides,axis=0)\n",
    "        return centroides\n",
    "    \n",
    "# a = GetCentersKMeans()\n",
    "# a((1, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "historicos = {}\n",
    "for neurons in range(204,271):\n",
    "    # creating RBFLayer with centers found by KMeans clustering\n",
    "    rbflayer = RBFLayer(neurons,\n",
    "                      initializer=GetCentersKMeans(),\n",
    "                      betas=0.5,\n",
    "                      input_shape=(30,))\n",
    "    model = tf.compat.v2.keras.Sequential()\n",
    "    model.add(rbflayer)\n",
    "    model.add(tf.compat.v2.keras.layers.Dense(units=27, activation='linear'))\n",
    "\n",
    "    # model.compile(loss='mean_squared_error',\n",
    "    #                   optimizer=RMSprop())\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=500, batch_size = 20, validation_data=(X_val, y_val), verbose=1)\n",
    "    loss_test, accuracy_test = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    lst_hist = history.history\n",
    "    tmp = [(lst_hist['acc'][-1],lst_hist['val_acc'][-1],accuracy_test), \n",
    "           (lst_hist['loss'][-1],lst_hist['val_loss'][-1],loss_test)]\n",
    "    historicos[neurons] = tmp\n",
    "    \n",
    "    if not os.path.exists('./variacao_rbfn_05'):\n",
    "        os.mkdir('./variacao_rbfn_05')\n",
    "    \n",
    "    with open('variacao_rbfn_05/'+ str(neurons) +'_history.pickle', 'wb') as fp:\n",
    "        pickle.dump(history.history, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('variacao_rbfn_05/'+ str(neurons) +'_tuplas_finais.pickle', 'wb') as fp:\n",
    "        pickle.dump(tmp, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    model.save('variacao_rbfn_05/'+ str(neurons) +'_network.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('rede_rbf.h5')\n",
    "\n",
    "# model = load_model(\"rede_rbf.h5\", custom_objects={'RBFLayer': RBFLayer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(1,21):\n",
    "    model = tf.compat.v2.keras.models.load_model(\"variacao_neuronios_relu/\"+str(n)+\"_network.h5\")\n",
    "\n",
    "    converter = tf.compat.v2.lite.TFLiteConverter.from_keras_model(model)\n",
    "    tflite_model = converter.convert()\n",
    "    file = open('mlp_models_lite_relu/' + str(n) + '_model_mlp.tflite' , 'wb' ) \n",
    "    file.write( tflite_model )\n",
    "    file.close()\n",
    "\n",
    "# interpreter = tf.lite.Interpreter(model_path=\"model_rbf.tflite\")\n",
    "# interpreter.allocate_tensors()\n",
    "\n",
    "# # Print input shape and type\n",
    "# print(interpreter.get_input_details()[0]['shape'])  # Example: [1 224 224 3]\n",
    "# print(interpreter.get_input_details()[0]['dtype'])  # Example: <class 'numpy.float32'>\n",
    "\n",
    "# # Print output shape and type\n",
    "# print(interpreter.get_output_details()[0]['shape'])  # Example: [1 1000]\n",
    "# print(interpreter.get_output_details()[0]['dtype'])  # Example: <class 'numpy.float32'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method RBFLayer.call of <__main__.RBFLayer object at 0x0000021C82F63308>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method RBFLayer.call of <__main__.RBFLayer object at 0x0000021C82F63308>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method RBFLayer.call of <__main__.RBFLayer object at 0x0000021C82F63308>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method RBFLayer.call of <__main__.RBFLayer object at 0x0000021C82F63308>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "model = tf.compat.v2.keras.models.load_model(\"variacao_rbfn_1/264_network.h5\", custom_objects={'RBFLayer': RBFLayer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6750/6750 [==============================] - 1s 114us/sample - loss: 2.6324e-04 - acc: 0.9984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.00026323610992619285, 0.99837035]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(27,271, 27):\n",
    "    model = tf.compat.v2.keras.models.load_model(\"variacao_rbfn_05/\"+str(n)+\"_network.h5\", custom_objects={'RBFLayer': RBFLayer})\n",
    "\n",
    "    converter = tf.compat.v2.lite.TFLiteConverter.from_keras_model(model)\n",
    "    tflite_model = converter.convert()\n",
    "    file = open('rbfn_05_models_lite/' + str(n) + '_model_rbfn.tflite' , 'wb' ) \n",
    "    file.write( tflite_model )\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB  \n",
    "NB = GaussianNB()  \n",
    "NB.fit(X_train, y_train)   \n",
    "# y_predict = NB.predict(X_test)  \n",
    "print(\"Accuracy NB: {:.4f}\".format(NB.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
